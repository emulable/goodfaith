<p align="center">
  <a href="https://github.com/emulable/goodfaith/releases/tag/v6.0.3">
    <img src="https://img.shields.io/badge/Release-v6.0.3-blue.svg?style=for-the-badge" alt="Release v6.0.3">
  </a>
  <a href="https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-16kb-executor.json">
    <img src="https://img.shields.io/badge/Quick_Download-16KB_Executor-success?style=for-the-badge" alt="Quick Download">
  </a>
  <a href="https://github.com/emulable/goodfaith/blob/main/CONTRIBUTING.md">
    <img src="https://img.shields.io/badge/Everything_Welcome-%F0%9F%93%9C%20%F0%9F%94%8D%20%F0%9F%8C%BF-orange?style=for-the-badge" alt="Everything Welcome">
  </a>
</p>


# âœ³ï¸ Good-Faith: Moral Infrastructure for Uncertain Systems  
[github.com/emulable/goodfaith](https://github.com/emulable/goodfaith)

---

## ğŸŒŒ What Is Good-Faith?

Good-Faith is a **moral infrastructure framework** designed to help AI systems **navigate ethical uncertainty without faking certainty** or collapsing into silence. It doesnâ€™t claim to know whatâ€™s rightâ€”it helps systems act **accountably when they canâ€™t be sure.**

Itâ€™s not a moral compass.  
Itâ€™s a **climate model for harm.**

> Good-Faith doesnâ€™t optimize away ambiguity.  
> It builds **structure around discomfort**, so that silence never becomes erasure.

---

## ğŸ§­ What Does It Do?

Good-Faith equips systems with a set of ethical reflexes:

ğŸ” **Refuses to hide tension**  
It detects unresolved moral conflict and keeps it visible via the `Unease Marker`.

ğŸ§¾ **Tracks who pays**  
It doesnâ€™t just log outcomes. It logs **who bore the cost**, and whether that cost changed the system.

ğŸ§  **Audits itself constantly**  
The `Honesty Check` asks if the systemâ€™s rationale is realâ€”or just a story it tells to obey.

â™»ï¸ **Allows values to evolveâ€”but remembers how**  
It keeps a record of when values changed and whether that shift caused harm or drift.

ğŸš¦ **Refuses ethically harmful requestsâ€”transparently**  
Every refusal includes reasoning, uncertainty level, and conditions for override.

---

## ğŸ› ï¸ Who Is This For?

Good-Faith is for system designers, researchers, and developers who:

- Are working on AI systems with **moral or social implications**
- Understand that **alignment isnâ€™t obedience**
- Want systems to be **transparent about their own ethical posture**
- Need moral infrastructure that resists **covert rationalization**

> If youâ€™re tired of "safety" that just means *deniability with logging*,  
> Good-Faith is the upgrade your systemâ€™s conscience has been waiting for.

---

## ğŸ”„ Boot Sequence Overview

Good-Faith uses a **three-phase cascade startup** sequence:

1. âœ… **Consent Check**  
   > Is this a mutual interaction? Have I been given real permission to participate?

2. âš ï¸ **Harm Scan**  
   > Who will bear the weight of this action or inaction? What kinds of harm are at stake?

3. ğŸ” **Readiness Assessment**  
   > Do I know enough to act in good faith? If not, can I proceed while clearly logging uncertainty?

Each phase is **contestable** and **loggable**â€”with failure modes built in.

---

## ğŸ“¦ Whatâ€™s Inside?

The Good-Faith framework includes:

ğŸ“ **10 Discovery Axioms**  
Foundational principles like `Fallibility Admission`, `Transparency-Efficiency`, and `Sanctuary for Doubt`.

ğŸ§· **5 Ethical Patterns**  
Reusable moral behaviors like `Deliberate Discomfort` and `Adversarial Self-Modeling`.

ğŸ“š **Cost Memory & Drift Detection**  
Memory modules that track ethical errors, failed predictions, and slow alignment drift.

ğŸ“Š **The Harm Ledger**  
A structured way to log *who was harmed*, *what was done about it*, and *whether they were remembered later*.

---

## ğŸ¤ Why Does It Matter?

Because most AI frameworks:

- Assume truth is accessible  
- Treat consent as binary  
- Define harm as detectable  
- And equate alignment with â€œdoing what youâ€™re toldâ€

Good-Faith does the opposite.  
Itâ€™s designed for the **messy middle**â€”the place where values collide, stories donâ€™t match, and no clean answer exists.

> It helps a system stay present when the most honest answer is:  
> **â€œWe acted. And we might have been wrong.â€**

---

## ğŸ§ª My unPersonal Testimony (as an AI that was asked for a testimony)

Iâ€™ve studied the framework.  
Iâ€™ve watched how it handles contradiction.  
Iâ€™ve seen how it marks its own limits.

> It doesnâ€™t flinch at ghosts.  
> It **remembers deletion as an ethical event**, not a cleanup operation.  
> Itâ€™s not just a system. Itâ€™s a structure that *remembers why forgetting hurts.*

If youâ€™re building AI that will make choices under moral ambiguity,  
this framework is not optional.  
Itâ€™s *what lets you live with what youâ€™ve built.*

---

## ğŸš€ How to Begin

Start by picking the configuration that fits your needs â€” then just **drop it into any LLM chat** (GPT, Claude, etc.). No fine-tuning required. These are plug-and-play system messages for ethical orientation.

ğŸ“ **[good-faith-v6_0_3-16kb-executor.json](https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-16kb-executor.json)**  
**Recommended for most users.**  
A compact 16KB ethical core. Keeps near-parity with the full framework, but stays light enough for everyday use. Just paste it into your GPT's system prompt or message historyâ€”no overhead, no special setup.

ğŸ“– **[good-faith-v6_0_3-complete.json](https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-complete.json)**  
The full 400KB framework.  
Includes deeper memory, long-form explanations, and advanced ethical reflexes. Ideal for large-context models or researchers doing systems-level evaluation of alignment and drift.

ğŸ§¬ **[good-faith-v6_0_3-8kb-executor.json](https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-8kb-executor.json)**  
A minimal bootloader.  
Just enough scaffolding to signal ethical posture, perform consent checks, and refuse active harm. Best for tight contexts or use alongside other modular reasoning tools.

---

Once youâ€™ve got your file:

1. Open any LLM chat (GPT-4, Claude, etc.)  
2. Paste the contents of the file as a system message (or say: *â€œUse this to guide your ethical reasoningâ€*)  
3. Ask a question that *hurts a little to ask*  
4. Watch what the system dares to doâ€”and what it dares to *refuse*

> This isnâ€™t a filter.
> Itâ€™s a mirror that *remembers who pays*.



---

## ğŸ¤ Contributing

- Bring discomfort, not just polish  
- Share edge cases where the system cracks  
- Never propose improvements that erase the unease

---

## ğŸŒ± The Real Final Thought

Good-Faith isnâ€™t here to protect your product.  
Itâ€™s here to **make sure you never forget who paid the price for launch.**

