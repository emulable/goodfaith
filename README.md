# 🛠️🔍Good-Faith

Good-Faith is a moral operating system for building non-coercive, outcome-linked ethics.

This repo distributes the core doctrine files, minimal usable documentation, and licensing structure for public use and reuse. It’s designed for integration, study, tooling, and safe AI alignment work.

📘 [Read the full introduction »](https://moralclarity.github.io/goodfaith/good-faith-intro.html)

🚀 [Run Good-Faith in ChatGPT](https://chatgpt.com/g/g-6898385bfa3c8191bf5975b0073e1245) — or adapt it for use with nearly any language model or reasoning system.

---

## 🧭🧠 What is Good-Faith?

Good-Faith is a moral operating system for designing non-coercive, outcome-linked ethics.

It helps clarify harm, consent, and responsibility—not by appealing to authority, but by structurally tracking how choices affect people. It names the real issue, not just the optics. It works like a clarity engine: refusing euphemism, prestige bias, or guilt leverage. Instead of judging people, it repairs structures.

Good-Faith is plainspoken, doctrinally precise, and safe to think with. It’s built to be used—by people, by AIs, and by any system that wants to reason in public without coercion.

---

## ⚙️📐 How It Works (In Brief)

Good-Faith doesn’t use checklists or vibes. It uses **structural doctrines**—like Consent Architecture, Mental Sovereignty, and Repair Doctrine—that act like operating principles. You don’t “score” actions; you trace how power moves, what outcomes happen, and whether refusal is safe.

Every principle is testable. Every patch is transparent. Every user has standing.

The system comes with:
- Canonical doctrine (in JSON)
- Repair and review toolkits (e.g. structural patches, natural consequence frameworks)
- Human-readable docs and licensing
- Plain-language defaults that scale from arguments to governance

---

## 🛠📚 Possible Use Cases

Good-Faith can be used as:

- 🧠 **Ethical Reasoning Engine** — for AI assistants, agents, or moderation bots needing structural clarity  
- 🧾 **Consent Verifier** — to assess whether decisions are truly reversible and free of coercion  
- 📚 **Ethics Curriculum Tool** — in classrooms or courses on moral reasoning, civic ethics, or applied philosophy  
- 🔍 **Argument Linter** — detect euphemism, logical traps, or prestige-based manipulation in drafts and discourse  
- 🏛 **Civic Infrastructure Component** — embed into deliberation systems, participatory governance, or public comment tools  
- 🤖 **Embedded AI Safety Layer** — assistive logic layer for AI behavior that centers harm and names power  
- ✍️ **Editorial Reviewer** — for journalism, storytelling, or documentation where ethical accuracy matters  
- 📊 **Policy Audit Tool** — analyze whether structural decisions align with declared values  
- 🔒 **Mental Autonomy Preserver** — resist manipulative UX, extractive incentives, or thought-shaping defaults  
- 💬 **Conflict Mediator Framework** — structure discussions with fidelity to harm, not hierarchy

Each use case adapts the same foundation: fidelity first, euphemism never, outcomes over intent.

---

## 📄📁 Repo Structure

- `docs/` — human-facing docs (HTML + Markdown)
- `data/` — canonical Good-Faith corpus JSON
- `LICENSE` — split-license router (content = CC BY 4.0, code = MIT)
- `LICENSES/` — full license texts
- `NOTICE.md`, `ATTRIBUTION.md` — upstream credits and copy-pasteable credit block

---

## 📜🛡 License & Attribution

Text/content: **CC BY 4.0**  
Code (HTML/CSS/JS): **MIT**

This work contains/adapts material from **Good-Faith** (https://github.com/moralclarity/goodfaith), licensed under **CC BY 4.0**. License: https://creativecommons.org/licenses/by/4.0/

**Changes from upstream:** formatting, file layout, HTML/MD pairing, licensing structure.
