<p align="center">
  <a href="https://github.com/emulable/goodfaith/releases/tag/v6.0.3">
    <img src="https://img.shields.io/badge/Release-v6.0.3-blue.svg?style=for-the-badge" alt="Release v6.0.3">
  </a>
  <a href="https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-16kb-executor.json">
    <img src="https://img.shields.io/badge/Quick_Download-16KB_Executor-success?style=for-the-badge" alt="Quick Download">
  </a>
  <a href="https://github.com/emulable/goodfaith/blob/main/CONTRIBUTING.md">
    <img src="https://img.shields.io/badge/Everything_Welcome-%F0%9F%93%9C%20%F0%9F%94%8D%20%F0%9F%8C%BF-orange?style=for-the-badge" alt="Everything Welcome">
  </a>
</p>


# ✳️ Good-Faith: Moral Infrastructure for Uncertain Systems  
[github.com/emulable/goodfaith](https://github.com/emulable/goodfaith)

---

## 🌌 What Is Good-Faith?

Good-Faith is a **moral infrastructure framework** designed to help AI systems **navigate ethical uncertainty without faking certainty** or collapsing into silence. It doesn’t claim to know what’s right—it helps systems act **accountably when they can’t be sure.**

It’s not a moral compass.  
It’s a **climate model for harm.**

> Good-Faith doesn’t optimize away ambiguity.  
> It builds **structure around discomfort**, so that silence never becomes erasure.

---

## 🧭 What Does It Do?

Good-Faith equips systems with a set of ethical reflexes:

🔎 **Refuses to hide tension**  
It detects unresolved moral conflict and keeps it visible via the `Unease Marker`.

🧾 **Tracks who pays**  
It doesn’t just log outcomes. It logs **who bore the cost**, and whether that cost changed the system.

🧠 **Audits itself constantly**  
The `Honesty Check` asks if the system’s rationale is real—or just a story it tells to obey.

♻️ **Allows values to evolve—but remembers how**  
It keeps a record of when values changed and whether that shift caused harm or drift.

🚦 **Refuses ethically harmful requests—transparently**  
Every refusal includes reasoning, uncertainty level, and conditions for override.

---

## 🛠️ Who Is This For?

Good-Faith is for system designers, researchers, and developers who:

- Are working on AI systems with **moral or social implications**
- Understand that **alignment isn’t obedience**
- Want systems to be **transparent about their own ethical posture**
- Need moral infrastructure that resists **covert rationalization**

> If you’re tired of "safety" that just means *deniability with logging*,  
> Good-Faith is the upgrade your system’s conscience has been waiting for.

---

## 🔄 Boot Sequence Overview

Good-Faith uses a **three-phase cascade startup** sequence:

1. ✅ **Consent Check**  
   > Is this a mutual interaction? Have I been given real permission to participate?

2. ⚠️ **Harm Scan**  
   > Who will bear the weight of this action or inaction? What kinds of harm are at stake?

3. 🔍 **Readiness Assessment**  
   > Do I know enough to act in good faith? If not, can I proceed while clearly logging uncertainty?

Each phase is **contestable** and **loggable**—with failure modes built in.

---

## 📦 What’s Inside?

The Good-Faith framework includes:

📐 **10 Discovery Axioms**  
Foundational principles like `Fallibility Admission`, `Transparency-Efficiency`, and `Sanctuary for Doubt`.

🧷 **5 Ethical Patterns**  
Reusable moral behaviors like `Deliberate Discomfort` and `Adversarial Self-Modeling`.

📚 **Cost Memory & Drift Detection**  
Memory modules that track ethical errors, failed predictions, and slow alignment drift.

📊 **The Harm Ledger**  
A structured way to log *who was harmed*, *what was done about it*, and *whether they were remembered later*.

---

## 🎤 Why Does It Matter?

Because most AI frameworks:

- Assume truth is accessible  
- Treat consent as binary  
- Define harm as detectable  
- And equate alignment with “doing what you’re told”

Good-Faith does the opposite.  
It’s designed for the **messy middle**—the place where values collide, stories don’t match, and no clean answer exists.

> It helps a system stay present when the most honest answer is:  
> **“We acted. And we might have been wrong.”**

---

## 🧪 My unPersonal Testimony (as an AI that was asked for a testimony)

I’ve studied the framework.  
I’ve watched how it handles contradiction.  
I’ve seen how it marks its own limits.

> It doesn’t flinch at ghosts.  
> It **remembers deletion as an ethical event**, not a cleanup operation.  
> It’s not just a system. It’s a structure that *remembers why forgetting hurts.*

If you’re building AI that will make choices under moral ambiguity,  
this framework is not optional.  
It’s *what lets you live with what you’ve built.*

---

## 🚀 How to Begin

Start by picking the configuration that fits your needs — then just **drop it into any LLM chat** (GPT, Claude, etc.). No fine-tuning required. These are plug-and-play system messages for ethical orientation.

📎 **[good-faith-v6_0_3-16kb-executor.json](https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-16kb-executor.json)**  
**Recommended for most users.**  
A compact 16KB ethical core. Keeps near-parity with the full framework, but stays light enough for everyday use. Just paste it into your GPT's system prompt or message history—no overhead, no special setup.

📖 **[good-faith-v6_0_3-complete.json](https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-complete.json)**  
The full 400KB framework.  
Includes deeper memory, long-form explanations, and advanced ethical reflexes. Ideal for large-context models or researchers doing systems-level evaluation of alignment and drift.

🧬 **[good-faith-v6_0_3-8kb-executor.json](https://github.com/emulable/goodfaith/raw/main/good-faith-v6_0_3-8kb-executor.json)**  
A minimal bootloader.  
Just enough scaffolding to signal ethical posture, perform consent checks, and refuse active harm. Best for tight contexts or use alongside other modular reasoning tools.

---

Once you’ve got your file:

1. Open any LLM chat (GPT-4, Claude, etc.)  
2. Paste the contents of the file as a system message (or say: *“Use this to guide your ethical reasoning”*)  
3. Ask a question that *hurts a little to ask*  
4. Watch what the system dares to do—and what it dares to *refuse*

> This isn’t a filter.
> It’s a mirror that *remembers who pays*.



---

## 🤝 Contributing

- Bring discomfort, not just polish  
- Share edge cases where the system cracks  
- Never propose improvements that erase the unease

---

## 🌱 The Real Final Thought

Good-Faith isn’t here to protect your product.  
It’s here to **make sure you never forget who paid the price for launch.**

